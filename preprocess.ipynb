{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('data/bandit_train.npy', allow_pickle=True)\n",
    "X_test = np.load('data/bandit_test.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'a', 1: 'b', 2: 'c', 3: 'd', 4: 'e', 5: 'f', 6: 'g', 7: 'h', 8: 'i', 9: 'j'}\n"
     ]
    }
   ],
   "source": [
    "action_mapper = {i: chr(i+97) for i in range(10)}\n",
    "print(action_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_start_token = \"Start: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The goal is to chose the action which maximizes the reward.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(ep):\n",
    "    rep = ep_start_token\n",
    "    for t in ep:\n",
    "        a, r= t\n",
    "        a = action_mapper[a]\n",
    "        rep += f\"{a} {r}\" + \" \"\n",
    "    return rep.rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_ar(ep):\n",
    "    rep = ep_start_token\n",
    "    for t in ep:\n",
    "        a, r= t\n",
    "        a = action_mapper[a]\n",
    "        rep += f\"action {a} reward {r}\" + \" \"\n",
    "    return rep.rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_normal(ep1, ep2):\n",
    "    rep = flatten(ep1) + \" \" + flatten(ep2)\n",
    "    return rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_explicit(ep1, ep2):\n",
    "    return flatten_ar(ep1) + \" \" + flatten_ar(ep2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_prompt(ep1, ep2):\n",
    "    return prompt + flatten_ar(ep1) + \" \" + flatten_ar(ep2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X, data_size=300, flattener=flatten_normal):\n",
    "    if X.shape[-1] == 3:\n",
    "        X = X[:, :, 1:]\n",
    "    indexes = []\n",
    "    while len(indexes) < data_size:\n",
    "        index = np.random.randint(0, 100, size=2).tolist()\n",
    "        if index not in indexes: indexes.append(index)\n",
    "\n",
    "    data = []\n",
    "    for i in range(len(indexes)):\n",
    "        i1, i2 = indexes[i]\n",
    "        ep1, ep2 = X[i1], X[i2]\n",
    "        rep = flattener(ep1, ep2)\n",
    "        data.append(rep)\n",
    "\n",
    "    df = pd.DataFrame({'history': data})\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sizes = [25, 100, 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for size in data_sizes:\n",
    "    flatteners = [(flatten_normal, 'normal')]\n",
    "    if size < 200:\n",
    "        flatteners += [(flatten_explicit, 'explicit'), (flatten_prompt, 'prompt')]\n",
    "    for flattener, fl_name in flatteners:\n",
    "        for typ in ['train', 'test']:\n",
    "            if typ == 'train': X = X_train\n",
    "            else: X = X_test\n",
    "\n",
    "            df = preprocess(X, data_size=size, flattener=flattener)\n",
    "            fname = f'data/{typ}_{fl_name}_{size}.csv'\n",
    "            df.to_csv(fname, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c821b3ac4508363ab26790ea772768a10c1768efd11bb9891ab119aba586fc55"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
